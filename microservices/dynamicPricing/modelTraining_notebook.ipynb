{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aQHcja9vOsED",
      "metadata": {
        "id": "aQHcja9vOsED"
      },
      "source": [
        "# Dynamic Pricing Neural Network Model Training\n",
        "\n",
        "The purpose of this notebook is to demonstrate how to train a TensorFlow neural network model for predicting an optimal price based on e-commerce events stored in a MongoDB Atlas feature store.\n",
        "\n",
        "Our ecommerce store has the following data model for capturing user behavior events:\n",
        "\n",
        "| Field         | Data Type | Description                                               | Example Values           |\n",
        "|---------------|-----------|-----------------------------------------------------------|--------------------------|\n",
        "| product_name  | String    | The name of the product                                   | \"MongoDB Notebook\"       |\n",
        "| product_id    | Integer   | Unique identifier for the product                         | 98803                    |\n",
        "| action        | String    | Type of action performed on the product (user interaction)| \"view\", \"add_to_cart\", \"purchase\" |\n",
        "| price         | Float     | Price of the product                                      | 18.99                    |\n",
        "| timestamp     | String    | ISO format timestamp of when the event occurred           | \"2024-03-25T12:36:25.428461\" |\n",
        "| encoded_name  | Integer   | An encoded version of the product name for machine learning models | 23363195            |\n",
        "\n",
        "This table assumes the product[\"price\"] field is a float representing the price of the product in a single currency (e.g., USD). The encoded_name field is considered an integer, which could represent a hash or an encoding used to transform the product name into a numerical format suitable for machine learning models. The timestamp field is a string formatted as an ISO timestamp, which provides the exact date and time when the action was recorded. The example values are placeholders and should be replaced with actual data from your application."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B5fddA9NQFnF",
      "metadata": {
        "id": "B5fddA9NQFnF"
      },
      "source": [
        "## Connecting to the MongoDB Database\n",
        "\n",
        "First, we need to install the necessary Python packages and establish a connection to our MongoDB database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780VBA0YENZ85M0r79Mcz4p4",
      "metadata": {
        "id": "780VBA0YENZ85M0r79Mcz4p4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "```python\n",
        "!pip install pymongo\n",
        "!pip install 'pymongo[srv]'\n",
        "!pip install pandas\n",
        "from pymongo import MongoClient\n",
        "import pandas as pd\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WolZyg0fOwNL",
      "metadata": {
        "id": "WolZyg0fOwNL"
      },
      "outputs": [],
      "source": [
        "# Replace the below connection string with your MongoDB connection URI\n",
        "conn_string = \"your_mongodb_connection_string_here\"\n",
        "client = MongoClient(conn_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qNlktX_hO5tE",
      "metadata": {
        "id": "qNlktX_hO5tE"
      },
      "outputs": [],
      "source": [
        "# Specify the database and collection\n",
        "db = client[\"your_database_name\"]\n",
        "collection = db[\"your_collection_name\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vDxQgr_mO8sb",
      "metadata": {
        "id": "vDxQgr_mO8sb"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Once connected, we'll fetch the data and perform some basic cleaning operations to prepare it for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AkxmPhXDO_Uv",
      "metadata": {
        "id": "AkxmPhXDO_Uv"
      },
      "outputs": [],
      "source": [
        "# Get all the documents\n",
        "documents = collection.find()\n",
        "\n",
        "# Convert the documents into a list and then into a DataFrame\n",
        "df = pd.DataFrame(list(documents))\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(columns=['product_name', 'product_id', 'timestamp', 'tensor'])\n",
        "\n",
        "# Extracting the 'amount' from the 'price' column and converting it to float\n",
        "df['price'] = df['price'].apply(lambda x: float(x['amount']) if isinstance(x, dict) and 'amount' in x else None)\n",
        "\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vbm3WoFPQMFd",
      "metadata": {
        "id": "Vbm3WoFPQMFd"
      },
      "source": [
        "## Building the Dynamic Pricing Model\n",
        "\n",
        "Next, we import the necessary TensorFlow and scikit-learn libraries, encode categorical variables, and normalize our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZlXb1QOFQQ7r",
      "metadata": {
        "id": "ZlXb1QOFQQ7r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DsH29AwhQb24",
      "metadata": {
        "id": "DsH29AwhQb24"
      },
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for column in ['action', 'encoded_name']:\n",
        "    le = LabelEncoder()\n",
        "    df[column] = le.fit_transform(df[column])\n",
        "    label_encoders[column] = le\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pZM8NhIaQgi5",
      "metadata": {
        "id": "pZM8NhIaQgi5"
      },
      "outputs": [],
      "source": [
        "# Standardizing\n",
        "scaler = StandardScaler()\n",
        "df[['action', 'encoded_name']] = scaler.fit_transform(df[['action', 'encoded_name']])\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fyGyjzscQwzV",
      "metadata": {
        "id": "fyGyjzscQwzV"
      },
      "source": [
        "## Saving the Encoder for Event Data Pre-processing\n",
        "\n",
        "We'll save the encoder objects to Google Cloud Storage for later use in preprocessing new data for predictions. This code will generate joblib files to save the encoding and standardizing criteria from the above preprocessing and upcoming training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U1HbOI6jQj8x",
      "metadata": {
        "id": "U1HbOI6jQj8x"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-storage\n",
        "from google.cloud import storage\n",
        "import joblib\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AwTRGD98RCq3",
      "metadata": {
        "id": "AwTRGD98RCq3"
      },
      "outputs": [],
      "source": [
        "# Initialize a client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# The name of your GCP bucket\n",
        "bucket_name = 'dyn_pricing_scaler'\n",
        "\n",
        "# The path within your bucket to save the scaler object\n",
        "destination_blob_name = 'labelEncoder.joblib'\n",
        "\n",
        "# Create a buffer\n",
        "buffer = io.BytesIO()\n",
        "\n",
        "# Dump the scaler object to the buffer\n",
        "joblib.dump(label_encoders, buffer)\n",
        "\n",
        "# Now upload the buffer content to GCS\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "# Rewind the buffer's file pointer to the beginning of the file\n",
        "buffer.seek(0)\n",
        "\n",
        "# Upload the contents of the buffer\n",
        "blob.upload_from_file(buffer, content_type='application/octet-stream')\n",
        "\n",
        "print(f\"Uploaded scaler to gs://{bucket_name}/{destination_blob_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vYYpujdDRI5g",
      "metadata": {
        "id": "vYYpujdDRI5g"
      },
      "outputs": [],
      "source": [
        "# Initialize a client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# The name of your GCP bucket\n",
        "bucket_name = 'dyn_pricing_scaler'\n",
        "\n",
        "# The path within your bucket to save the scaler object\n",
        "destination_blob_name = 'scaler.joblib'\n",
        "\n",
        "# Create a buffer\n",
        "buffer = io.BytesIO()\n",
        "\n",
        "# Dump the scaler object to the buffer\n",
        "joblib.dump(scaler, buffer)\n",
        "\n",
        "# Now upload the buffer content to GCS\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "# Rewind the buffer's file pointer to the beginning of the file\n",
        "buffer.seek(0)\n",
        "\n",
        "# Upload the contents of the buffer\n",
        "blob.upload_from_file(buffer, content_type='application/octet-stream')\n",
        "\n",
        "print(f\"Uploaded scaler to gs://{bucket_name}/{destination_blob_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mLn0d2VcROTY",
      "metadata": {
        "id": "mLn0d2VcROTY"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "With our data prepared, we'll split it into training and testing sets, define our neural network architecture, and train our model. Please remind this is a model meant for a simple demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AzTMe-TxRSAJ",
      "metadata": {
        "id": "AzTMe-TxRSAJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Print the model summary to check the architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_split=0.2,  # Further split the training set for validation\n",
        "                    epochs=10,  # Number of epochs to train for\n",
        "                    verbose=1,  # Show training output\n",
        "                    )\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "veKHble-RtsB",
      "metadata": {
        "id": "veKHble-RtsB"
      },
      "source": [
        "## Test Prediction\n",
        "\n",
        "After training, we make a test prediction to verify the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sjN-VZvlRx5x",
      "metadata": {
        "id": "sjN-VZvlRx5x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example new data point converted into a tensor\n",
        "new_data = np.array([[1.225999, -0.957973]])\n",
        "\n",
        "predicted_price = model.predict(new_data)\n",
        "print(\"Predicted Price:\", predicted_price[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sdvr_1kbR-ys",
      "metadata": {
        "id": "Sdvr_1kbR-ys"
      },
      "source": [
        "## Saving the Model\n",
        "\n",
        "Finally, we'll save our trained model to Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BIx4T-m4SC2E",
      "metadata": {
        "id": "BIx4T-m4SC2E"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'your-gcp-project-id'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "model_dir = 'your-model-directory'\n",
        "\n",
        "# Save the model to GCS\n",
        "model_dir = f'gs://{bucket_name}/{model_dir}\n",
        "model.save(model_dir)\n",
        "\n",
        "bucket_name = 'your-cloud-storage-bucket-name'\n",
        "model_dir = 'your-model-directory'\n",
        "model_path = 'your-model-path'\n",
        "!gsutil mb -l us-central1 gs://{bucket_name}  # Create the bucket if necessary\n",
        "!gsutil cp -r {model_dir} gs://{bucket_name}/{model_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fi3pn-FhSmGk",
      "metadata": {
        "id": "Fi3pn-FhSmGk"
      },
      "source": [
        "Next we'll register our trained model in the VertexAI model registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uwXYBK1mStTz",
      "metadata": {
        "id": "uwXYBK1mStTz"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project='your-gcp-project-id', location='your-gcp-region')\n",
        "\n",
        "#Model registry\n",
        "\n",
        "model_display_name = 'dyn_pricingv1'\n",
        "model_description = 'TensorFlow dynamic pricing model'\n",
        "bucket_name = 'your-gcp-bucket-name'\n",
        "model_path = 'your-model-path'\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=model_display_name,\n",
        "    artifact_uri=f'gs://{bucket_name}/{model_path}',\n",
        "    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest',\n",
        "    description=model_description,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ymVCMy2TLDk",
      "metadata": {
        "id": "3ymVCMy2TLDk"
      },
      "source": [
        "After this you should be able to see your model in the VertexAI model registry GUI. You can follow [this guide](https://cloud.google.com/vertex-ai/docs/general/deployment) to deploy the model to a live endpoint.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "francesco.baldissera (Apr 3, 2024, 3:50:53â€¯PM)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
